<!DOCTYPE html SYSTEM "about:legacy-compat">
<html lang="en-US" data-preset="contrast" data-primary-color="#307FFF"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="UTF-8"><meta name="robots" content="noindex"><meta name="built-on" content="2024-04-22T11:03:06.093986"><title>How LLM Works | Evolany Tech Team</title><script type="application/json" id="virtual-toc-data">[{"id":"pre-training-base-model","level":0,"title":"Pre-training: Base Model","anchor":"#pre-training-base-model"},{"id":"fine-tuning-train-the-assistant","level":0,"title":"Fine-tuning: Train the Assistant","anchor":"#fine-tuning-train-the-assistant"},{"id":"rlhf-reinforcement-learning-from-human-feedback","level":0,"title":"RLHF: Reinforcement Learning from Human Feedback","anchor":"#rlhf-reinforcement-learning-from-human-feedback"},{"id":"prompt-engineering","level":0,"title":"Prompt Engineering","anchor":"#prompt-engineering"}]</script><script type="application/json" id="topic-shortcuts"></script><link href="https://resources.jetbrains.com/writerside/apidoc/6.6.6-b224/app.css" rel="stylesheet"><link rel="icon" type="image/svg" sizes="16x16" href="images/evolany-logo-svg.svg"><meta name="image" content=""><!-- Open Graph --><meta property="og:title" content="How LLM Works | Evolany Tech Team"><meta property="og:description" content=""><meta property="og:image" content=""><meta property="og:site_name" content="Evolany Tech Team Help"><meta property="og:type" content="website"><meta property="og:locale" content="en_US"><meta property="og:url" content="writerside-documentation/how-llm-works.html"><!-- End Open Graph --><!-- Twitter Card --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content=""><meta name="twitter:title" content="How LLM Works | Evolany Tech Team"><meta name="twitter:description" content=""><meta name="twitter:creator" content=""><meta name="twitter:image:src" content=""><!-- End Twitter Card --><!-- Schema.org WebPage --><script type="application/ld+json">{
    "@context": "http://schema.org",
    "@type": "WebPage",
    "@id": "writerside-documentation/how-llm-works.html#webpage",
    "url": "writerside-documentation/how-llm-works.html",
    "name": "How LLM Works | Evolany Tech Team",
    "description": "",
    "image": "",
    "inLanguage":"en-US"
}</script><!-- End Schema.org --><!-- Schema.org WebSite --><script type="application/ld+json">{
    "@type": "WebSite",
    "@id": "writerside-documentation/#website",
    "url": "writerside-documentation/",
    "name": "Evolany Tech Team Help"
}</script><!-- End Schema.org --></head><body data-id="How-LLM-Works" data-main-title="How LLM Works" data-article-props="{&quot;seeAlsoStyle&quot;:&quot;links&quot;}" data-template="article" data-breadcrumbs="Artificial-Intelligence.md|Machine Learning Progress///Intro-to-LLMs.md|Intro to LLMs"><div class="wrapper"><main class="panel _main"><header class="panel__header"><div class="container"><h3>Evolany Tech Team  Help</h3><div class="panel-trigger"></div></div></header><section class="panel__content"><div class="container"><article class="article" data-shortcut-switcher="inactive"><h1 data-toc="How-LLM-Works" id="How-LLM-Works.md">How LLM Works</h1><ul class="list _bullet" id="h7gh7y_2"><li class="list__item" id="h7gh7y_3"><ol class="list _decimal" id="h7gh7y_4" type="1"><li class="list__item" id="h7gh7y_5"><p>A document <span class="emphasis" id="h7gh7y_6"><span class="control" id="h7gh7y_7">completer</span></span> model works like this:</p></li></ol></li></ul><svg aria-roledescription="flowchart-v2" role="graphics-document document" viewBox="-8 -8 431.703125 96.3125" height="96.3125" xmlns="http://www.w3.org/2000/svg" width="431.703125" id="mermaid"><g><marker orient="auto" markerHeight="12" markerWidth="12" markerUnits="userSpaceOnUse" refY="5" refX="10" viewBox="0 0 12 20" class="marker flowchart" id="flowchart-pointEnd"><path style="stroke-width: 1; stroke-dasharray: 1, 0;" class="arrowMarkerPath" d="M 0 0 L 10 5 L 0 10 z"></path></marker><marker orient="auto" markerHeight="12" markerWidth="12" markerUnits="userSpaceOnUse" refY="5" refX="0" viewBox="0 0 10 10" class="marker flowchart" id="flowchart-pointStart"><path style="stroke-width: 1; stroke-dasharray: 1, 0;" class="arrowMarkerPath" d="M 0 5 L 10 10 L 10 0 z"></path></marker><marker orient="auto" markerHeight="11" markerWidth="11" markerUnits="userSpaceOnUse" refY="5" refX="11" viewBox="0 0 10 10" class="marker flowchart" id="flowchart-circleEnd"><circle style="stroke-width: 1; stroke-dasharray: 1, 0;" class="arrowMarkerPath" r="5" cy="5" cx="5"></circle></marker><marker orient="auto" markerHeight="11" markerWidth="11" markerUnits="userSpaceOnUse" refY="5" refX="-1" viewBox="0 0 10 10" class="marker flowchart" id="flowchart-circleStart"><circle style="stroke-width: 1; stroke-dasharray: 1, 0;" class="arrowMarkerPath" r="5" cy="5" cx="5"></circle></marker><marker orient="auto" markerHeight="11" markerWidth="11" markerUnits="userSpaceOnUse" refY="5.2" refX="12" viewBox="0 0 11 11" class="marker cross flowchart" id="flowchart-crossEnd"><path style="stroke-width: 2; stroke-dasharray: 1, 0;" class="arrowMarkerPath" d="M 1,1 l 9,9 M 10,1 l -9,9"></path></marker><marker orient="auto" markerHeight="11" markerWidth="11" markerUnits="userSpaceOnUse" refY="5.2" refX="-1" viewBox="0 0 11 11" class="marker cross flowchart" id="flowchart-crossStart"><path style="stroke-width: 2; stroke-dasharray: 1, 0;" class="arrowMarkerPath" d="M 1,1 l 9,9 M 10,1 l -9,9"></path></marker><g class="root"><g class="clusters"></g><g class="edgePaths"><path marker-end="url(#flowchart-pointEnd)" style="fill:none;" class="edge-thickness-normal edge-pattern-solid flowchart-link LS-head LE-torso" id="L-head-torso-0" d="M66.515625,40.15625L73.42057291666667,40.15625C80.32552083333333,40.15625,94.13541666666667,40.15625,107.9453125,40.15625C121.75520833333333,40.15625,135.56510416666666,40.15625,142.47005208333334,40.15625L149.375,40.15625"></path><path marker-end="url(#flowchart-pointEnd)" style="fill:none;" class="edge-thickness-normal edge-pattern-solid flowchart-link LS-torso LE-tail" id="L-torso-tail-0" d="M252.53125,40.15625L259.4361979166667,40.15625C266.3411458333333,40.15625,280.1510416666667,40.15625,293.9609375,40.15625C307.7708333333333,40.15625,321.5807291666667,40.15625,328.4856770833333,40.15625L335.390625,40.15625"></path></g><g class="edgeLabels"><g transform="translate(107.9453125, 40.15625)" class="edgeLabel"><g transform="translate(-16.4296875, -9.25)" class="label"><foreignObject height="18.5" width="32.859375"><div style="display: inline-block; white-space: nowrap;" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel">send</span></div></foreignObject></g></g><g transform="translate(293.9609375, 40.15625)" class="edgeLabel"><g transform="translate(-16.4296875, -9.25)" class="label"><foreignObject height="18.5" width="32.859375"><div style="display: inline-block; white-space: nowrap;" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel">send</span></div></foreignObject></g></g></g><g class="nodes"><g transform="translate(33.2578125, 40.15625)" id="flowchart-head-6" class="node default default flowchart-label"><circle height="33.5" width="66.515625" r="33.2578125" ry="0" rx="0" style=""></circle><g transform="translate(-25.7578125, -9.25)" style="" class="label"><rect></rect><foreignObject height="18.5" width="51.515625"><div style="display: inline-block; white-space: nowrap;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel">Prompt</span></div></foreignObject></g></g><g transform="translate(200.953125, 40.15625)" id="flowchart-torso-7" class="node default default flowchart-label"><rect height="33.5" width="103.15625" y="-16.75" x="-51.578125" ry="16.75" rx="16.75" style=""></rect><g transform="translate(-39.890625, -9.25)" style="" class="label"><rect></rect><foreignObject height="18.5" width="79.78125"><div style="display: inline-block; white-space: nowrap;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel">Base Model</span></div></foreignObject></g></g><g transform="translate(375.546875, 40.15625)" id="flowchart-tail-8" class="node default default flowchart-label"><circle height="33.5" width="80.3125" r="40.15625" ry="0" rx="0" style=""></circle><g transform="translate(-32.65625, -9.25)" style="" class="label"><rect></rect><foreignObject height="18.5" width="65.3125"><div style="display: inline-block; white-space: nowrap;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel">Response</span></div></foreignObject></g></g></g></g></g></svg><p id="h7gh7y_9"><span class="emphasis" id="h7gh7y_10">user prompt:</span></p><div class="code-block" data-lang="plaintext">
A banana is
</div><p id="h7gh7y_12"><span class="emphasis" id="h7gh7y_13">model response:</span></p><div class="code-block" data-lang="plaintext">
an elongated, edible fruit
</div><ul class="list _bullet" id="h7gh7y_15"><li class="list__item" id="h7gh7y_16"><ol class="list _decimal" id="h7gh7y_17" type="1" start="2"><li class="list__item" id="h7gh7y_18"><p>A document <span class="emphasis" id="h7gh7y_19"><span class="control" id="h7gh7y_20">generator</span></span> model works like this:</p></li></ol></li></ul><svg aria-roledescription="flowchart-v2" role="graphics-document document" viewBox="-8 -8 534.03125 96.3125" height="96.3125" xmlns="http://www.w3.org/2000/svg" width="534.03125" id="mermaid"><g><marker orient="auto" markerHeight="12" markerWidth="12" markerUnits="userSpaceOnUse" refY="5" refX="10" viewBox="0 0 12 20" class="marker flowchart" id="flowchart-pointEnd"><path style="stroke-width: 1; stroke-dasharray: 1, 0;" class="arrowMarkerPath" d="M 0 0 L 10 5 L 0 10 z"></path></marker><marker orient="auto" markerHeight="12" markerWidth="12" markerUnits="userSpaceOnUse" refY="5" refX="0" viewBox="0 0 10 10" class="marker flowchart" id="flowchart-pointStart"><path style="stroke-width: 1; stroke-dasharray: 1, 0;" class="arrowMarkerPath" d="M 0 5 L 10 10 L 10 0 z"></path></marker><marker orient="auto" markerHeight="11" markerWidth="11" markerUnits="userSpaceOnUse" refY="5" refX="11" viewBox="0 0 10 10" class="marker flowchart" id="flowchart-circleEnd"><circle style="stroke-width: 1; stroke-dasharray: 1, 0;" class="arrowMarkerPath" r="5" cy="5" cx="5"></circle></marker><marker orient="auto" markerHeight="11" markerWidth="11" markerUnits="userSpaceOnUse" refY="5" refX="-1" viewBox="0 0 10 10" class="marker flowchart" id="flowchart-circleStart"><circle style="stroke-width: 1; stroke-dasharray: 1, 0;" class="arrowMarkerPath" r="5" cy="5" cx="5"></circle></marker><marker orient="auto" markerHeight="11" markerWidth="11" markerUnits="userSpaceOnUse" refY="5.2" refX="12" viewBox="0 0 11 11" class="marker cross flowchart" id="flowchart-crossEnd"><path style="stroke-width: 2; stroke-dasharray: 1, 0;" class="arrowMarkerPath" d="M 1,1 l 9,9 M 10,1 l -9,9"></path></marker><marker orient="auto" markerHeight="11" markerWidth="11" markerUnits="userSpaceOnUse" refY="5.2" refX="-1" viewBox="0 0 11 11" class="marker cross flowchart" id="flowchart-crossStart"><path style="stroke-width: 2; stroke-dasharray: 1, 0;" class="arrowMarkerPath" d="M 1,1 l 9,9 M 10,1 l -9,9"></path></marker><g class="root"><g class="clusters"></g><g class="edgePaths"><path marker-end="url(#flowchart-pointEnd)" style="fill:none;" class="edge-thickness-normal edge-pattern-solid flowchart-link LS-head LE-torso" id="L-head-torso-0" d="M66.515625,40.15625L73.42057291666667,40.15625C80.32552083333333,40.15625,94.13541666666667,40.15625,107.9453125,40.15625C121.75520833333333,40.15625,135.56510416666666,40.15625,142.47005208333334,40.15625L149.375,40.15625"></path><path marker-end="url(#flowchart-pointEnd)" style="fill:none;" class="edge-thickness-normal edge-pattern-solid flowchart-link LS-torso LE-tail" id="L-torso-tail-0" d="M354.859375,40.15625L361.7643229166667,40.15625C368.6692708333333,40.15625,382.4791666666667,40.15625,396.2890625,40.15625C410.0989583333333,40.15625,423.9088541666667,40.15625,430.8138020833333,40.15625L437.71875,40.15625"></path></g><g class="edgeLabels"><g transform="translate(107.9453125, 40.15625)" class="edgeLabel"><g transform="translate(-16.4296875, -9.25)" class="label"><foreignObject height="18.5" width="32.859375"><div style="display: inline-block; white-space: nowrap;" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel">send</span></div></foreignObject></g></g><g transform="translate(396.2890625, 40.15625)" class="edgeLabel"><g transform="translate(-16.4296875, -9.25)" class="label"><foreignObject height="18.5" width="32.859375"><div style="display: inline-block; white-space: nowrap;" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel">send</span></div></foreignObject></g></g></g><g class="nodes"><g transform="translate(33.2578125, 40.15625)" id="flowchart-head-15" class="node default default flowchart-label"><circle height="33.5" width="66.515625" r="33.2578125" ry="0" rx="0" style=""></circle><g transform="translate(-25.7578125, -9.25)" style="" class="label"><rect></rect><foreignObject height="18.5" width="51.515625"><div style="display: inline-block; white-space: nowrap;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel">Prompt</span></div></foreignObject></g></g><g transform="translate(252.1171875, 40.15625)" id="flowchart-torso-16" class="node default default flowchart-label"><rect height="33.5" width="205.484375" y="-16.75" x="-102.7421875" ry="16.75" rx="16.75" style=""></rect><g transform="translate(-91.0546875, -9.25)" style="" class="label"><rect></rect><foreignObject height="18.5" width="182.109375"><div style="display: inline-block; white-space: nowrap;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel">Fine-tuned &amp; RLHF Model</span></div></foreignObject></g></g><g transform="translate(477.875, 40.15625)" id="flowchart-tail-17" class="node default default flowchart-label"><circle height="33.5" width="80.3125" r="40.15625" ry="0" rx="0" style=""></circle><g transform="translate(-32.65625, -9.25)" style="" class="label"><rect></rect><foreignObject height="18.5" width="65.3125"><div style="display: inline-block; white-space: nowrap;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel">Response</span></div></foreignObject></g></g></g></g></g></svg><p id="h7gh7y_22"><span class="emphasis" id="h7gh7y_23">user prompt:</span></p><div class="code-block" data-lang="plaintext">
I want to buy a new car
</div><p id="h7gh7y_25"><span class="emphasis" id="h7gh7y_26">model response:</span></p><div class="code-block" data-lang="plaintext">
What kind of car do you want to buy?
</div><p id="h7gh7y_28">Note the differences between the two above.</p><p id="h7gh7y_29">First model is just a document completer where it will only complete the prompt with what it finds with the highest possibility to become the next character. This is the model which we trained on the chunk of internet data, it's called the <span class="control" id="h7gh7y_30">base model</span>.</p><p id="h7gh7y_31">The second model is a document generator where it will generate a response <span class="emphasis" id="h7gh7y_32">more like a human</span> based on the prompt question. This is the <span class="control" id="h7gh7y_33">ChatGPT model</span>.</p><p id="h7gh7y_34">The ChatGPT model is an inference model that can generate a response based on the prompt question. I'll say its 99% the base model, but with two extra steps of training: A <span class="control" id="h7gh7y_35">fine-tuning</span> step and a <span class="control" id="h7gh7y_36">reinforcement learning from human feedback</span> step.</p><section class="chapter"><h2 id="pre-training-base-model" data-toc="pre-training-base-model">Pre-training: Base Model</h2><p id="h7gh7y_37">This constitutes the very core of the AI revolution and is where the magic truly lies.</p><p id="h7gh7y_38">Training a model is a process of feeding it with a lot of data and let it learn from it.</p><p id="h7gh7y_39">As described in the <a href="https://arxiv.org/abs/2005.14165" id="h7gh7y_40" data-external="true" rel="noopener noreferrer">GPT-3 paper</a>, the base model is trained on a large chunk of internet data. That's not an easy task for any individuals like you and me. It not only requires obtaining the data, but also requires a lot of computing power like GPU and TPU.</p><p id="h7gh7y_41"><span class="emphasis" id="h7gh7y_42">But don't worry, we can still learn to train a small GPT model on our own computer. I'll show you how to do it in the next topic.</span></p><p id="h7gh7y_43">The innovation behind LLM training lies in the introduction of the <span class="control" id="h7gh7y_44">Transformer architecture</span>, which enables the model to learn from vast quantities of data while preserving crucial <span class="emphasis" id="h7gh7y_45">contextual relationships</span> between different parts of the input.</p><p id="h7gh7y_46">By maintaining these connections, the model can effectively infer new insights based on the provided contexts, whether they be individual words, sentences, paragraphs, or beyond. With this capability, LLM training has opened up new opportunities for natural language processing and generation tasks, allowing machines to better understand and respond to human communication.</p><p id="h7gh7y_47">The transformer architecture used to train the base model is shown below:</p><figure id="h7gh7y_48"><img alt="Transformer paper architecture" src="images/transformer-paper-architecture.png" title="Transformer paper architecture" width="400" height="521"></figure><p id="h7gh7y_49">This is a neural-network-based model training with some old and new techniques: <code class="code" id="h7gh7y_50">tokenization</code>, <code class="code" id="h7gh7y_51">embedding</code>, <code class="code" id="h7gh7y_52">position encoding</code>, <code class="code" id="h7gh7y_53">feed-forward</code>, <code class="code" id="h7gh7y_54">normalization</code>, <code class="code" id="h7gh7y_55">softmax</code>, <code class="code" id="h7gh7y_56">linear transformation</code>, and most importantly, <code class="code" id="h7gh7y_57">multi-head attention</code>.</p><p id="h7gh7y_58">This part is that you and me are all mostly interested in. We want to clearly understand the idea behind the architecture and how exactly the training was done. So from next topic and beyond, we will start dig into the paper, code and mathematics that used in training the base model.</p></section><section class="chapter"><h2 id="fine-tuning-train-the-assistant" data-toc="fine-tuning-train-the-assistant">Fine-tuning: Train the Assistant</h2><p id="h7gh7y_59">Fine-tuning is a very smart implementation. I guess it's first done by OpenAI. The idea is super simple but works intelligently: hire human labelers to create lots of Q&amp;A conversation pairs (like 100k conversations). Then feed the model with the conversation pairs and let it learn from it.</p><p id="h7gh7y_60">This process is called <span class="emphasis" id="h7gh7y_61"><span class="control" id="h7gh7y_62">Fine-tuning</span></span>. You know what happens after those 100k sample conversations are trained into the model? The model will start response like a human!</p><p id="h7gh7y_63">Let's take a look at those sample labeled conversations:</p><dl id="h7gh7y_64" data-style="title-top"><dt id="h7gh7y_65" data-expandable="false">Human labeled Q&amp;A</dt><dd><p id="h7gh7y_67">Q: What is your name?</p><p id="h7gh7y_68">A: My name is John.</p></dd><dt id="h7gh7y_69" data-expandable="false">Human labeled Q&amp;A</dt><dd><p id="h7gh7y_71">Q: What's the capital of China?</p><p id="h7gh7y_72">A: China's capital is Beijing.</p></dd><dt id="h7gh7y_73" data-expandable="false">Human labeled Q&amp;A</dt><dd><p id="h7gh7y_75">Q: Summarize the plot of the movie Titanic.</p><p id="h7gh7y_76">A: The movie Titanic is about a ship that sinks in the ocean.</p></dd></dl><p id="h7gh7y_77">Whoa, these sample Q&amp;As are mocking the way we talk to each other.</p><p id="h7gh7y_78">By teaching the model these respond styles, the probability of the related contextual response will become very high and become response to a user's prompt. Through training the model in various conversational styles, we increase the likelihood that it will provide relevant and contextually appropriate responses to prompts.</p><p id="h7gh7y_79">This is how language models can appear so intelligent and human-like; by learning to mimic the rhythms and patterns of real-world conversations, they can convincingly simulate a back-and-forth dialogue with users.</p><p id="h7gh7y_80">At this step, we can say we obtained an <span class="emphasis" id="h7gh7y_81"><span class="control" id="h7gh7y_82">Assistant Model</span></span>.</p><p id="h7gh7y_83">Below is a diagram of showing some highlights from pre-training the Base Model to the Fine-tuning the Assistant Model:</p><figure id="h7gh7y_84"><img alt="Concept base fintune" src="images/concept-base-fintune.png" title="Concept base fintune" width="600" height="304"></figure><p id="h7gh7y_85">(from <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY" id="h7gh7y_86" data-external="true" rel="noopener noreferrer">Andrej Karpathy's build a GPT model from scratch)</a>)</p></section><section class="chapter"><h2 id="rlhf-reinforcement-learning-from-human-feedback" data-toc="rlhf-reinforcement-learning-from-human-feedback">RLHF: Reinforcement Learning from Human Feedback</h2><p id="h7gh7y_87">On January 2022 OpenAI published their works on <a href="https://openai.com/research/instruction-following" id="h7gh7y_88" data-external="true" rel="noopener noreferrer">Aligning language models to follow instructions</a>. In their blog post they describe how the model was futhur fine-tuned with human feedback:</p><figure id="h7gh7y_89"><img alt="Introduct chat gpt" src="images/introduct-chatGPT.png" title="Introduct chat gpt" width="1652" height="1042"></figure><p id="h7gh7y_90">This one is a bit tricky. The idea is to let the model learn from human feedback. Instead of providing ~100k labeled Q&amp;A pairs, they gather user's prompts and model responses, then let human rank them. Having the ranked conversations as the most-desired Q&amp;A samples, then feed them to the model again and let it learn from it to improve its overall performance.</p><p id="h7gh7y_91">This process is introduced by OpenAI on its <a href="https://openai.com/research/instruction-following" id="h7gh7y_92" data-external="true" rel="noopener noreferrer">blog</a>:</p><aside class="prompt" data-type="tip" data-title="" id="h7gh7y_93"><p id="h7gh7y_94">To make our models safer, more helpful, and more aligned, we use an existing technique called reinforcement learning from human feedback (RLHF). On prompts submitted by our customers to the API, our labelers provide demonstrations of the desired model behavior, and rank several outputs from our models. We then use this data to fine-tune GPT-3.</p></aside><p id="h7gh7y_95">Here is a base model vs fine-tuned/RLHF response comparison:</p><div class="table-wrapper"><table class="wide" id="h7gh7y_96"><thead><tr class="ijRowHead" id="h7gh7y_97"><th id="h7gh7y_98"><p>prompt:</p></th><th id="h7gh7y_99"><p><span class="emphasis" id="h7gh7y_100"><span class="control" id="h7gh7y_101">Explain the moon landing to a 6 year old in a few sentences.</span></span></p></th></tr></thead><tbody><tr id="h7gh7y_102"><td id="h7gh7y_103"><p><code class="code" id="h7gh7y_104">Base</code></p></td><td id="h7gh7y_105"><p><span class="emphasis" id="h7gh7y_106">Explain the theory of gravity to a 6 year old.</span></p></td></tr><tr id="h7gh7y_107"><td id="h7gh7y_108"></td><td id="h7gh7y_109"><p><span class="emphasis" id="h7gh7y_110">Explain the theory of relativity to a 6 year old in a few sentences.</span></p></td></tr><tr id="h7gh7y_111"><td id="h7gh7y_112"></td><td id="h7gh7y_113"><p><span class="emphasis" id="h7gh7y_114">Explain the big bang theory to a 6 year old.</span></p></td></tr><tr id="h7gh7y_115"><td id="h7gh7y_116"></td><td id="h7gh7y_117"><p><span class="emphasis" id="h7gh7y_118">Explain evolution to a 6 year old.</span></p></td></tr><tr id="h7gh7y_119"><td id="h7gh7y_120"><p><code class="code" id="h7gh7y_121">Fine-tuned/RLHF</code></p></td><td id="h7gh7y_122"><p><span class="emphasis" id="h7gh7y_123">People went to the moon, and they took pictures of what they saw, and sent them back to the earth so we could all see them.</span></p></td></tr></tbody></table></div><p id="h7gh7y_124">You can see without fine-tune and RLHF, the model is just a document completer.</p></section><section class="chapter"><h2 id="prompt-engineering" data-toc="prompt-engineering">Prompt Engineering</h2><p id="h7gh7y_125">Even with the fine-tuning and RLHF, the model still needs some help to get the desired response. And this is where the prompt engineering comes in.</p><p id="h7gh7y_126">In simple words, we can carefully design the prompt to get the desired response from the model (sometimes even without fine-tuning).</p><p id="h7gh7y_127">If you are not trying to dive too much into the mathematics and code, then <span class="control" id="h7gh7y_128">prompt engineering</span> is the good way to pay more attention, because it can get the best out of an LLM model simply by typing a better prompt.</p><p id="h7gh7y_129">Now let's look at an example:</p><p id="h7gh7y_130"><span class="emphasis" id="h7gh7y_131">prompt:</span></p><div class="code-block" data-lang="plaintext">
The sky is
</div><p id="h7gh7y_133"><span class="emphasis" id="h7gh7y_134">output:</span></p><div class="code-block" data-lang="plaintext">
blue.
</div><p id="h7gh7y_136"><span class="control" id="h7gh7y_137">Let's try to improve it a bit:</span></p><p id="h7gh7y_138"><span class="emphasis" id="h7gh7y_139">prompt:</span></p><div class="code-block" data-lang="plaintext">
Complete the sentence: 
The sky is
</div><p id="h7gh7y_141"><span class="emphasis" id="h7gh7y_142">output:</span></p><div class="code-block" data-lang="plaintext">
blue during the day and dark at night.
</div><p id="h7gh7y_144">By including some instructions in the prompt, the model will know what to do and what to response.</p><p id="h7gh7y_145"><span class="control" id="h7gh7y_146">Let's look at another interesting example:</span></p><p id="h7gh7y_147"><span class="emphasis" id="h7gh7y_148">prompt:</span></p><div class="code-block" data-lang="plaintext">
When I was 6 my sister was half my age. Now
I’m 70 how old is my sister?
</div><p id="h7gh7y_150"><span class="emphasis" id="h7gh7y_151">output:</span></p><div class="code-block" data-lang="plaintext">
35
</div><p id="h7gh7y_153"><span class="control" id="h7gh7y_154">The answer is wrong.</span> Correct answer should be 67. It looks like the model understands the questions but refers to a math calculation instead of logical inference.</p><p id="h7gh7y_155">Without fine-tuning and RLHF, we can get the correct answer solely by adding more example instructions to the prompt:</p><p id="h7gh7y_156"><span class="emphasis" id="h7gh7y_157">prompt:</span></p><div class="code-block" data-lang="plaintext">
Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,
there will be 21 trees. How many trees did the grove workers plant today?
A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted.
So, they must have planted 21 - 15 = 6 trees. The answer is 6.
Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?
A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.
Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?
A: Leah had 32 chocolates and Leah’s sister had 42. That means there were originally 32 + 42 = 74
chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.
Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops
did Jason give to Denny?
A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of
lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.
Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does
he have now?
A: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so
in total he has 7 + 2 = 9 toys. The answer is 9.
Q: There were nine computers in the server room. Five more computers were installed each day, from
monday to thursday. How many computers are now in the server room?
A: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 =
20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers.
The answer is 29.
Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many
golf balls did he have at the end of wednesday?
A: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On
Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.
Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?
A: She bought 5 bagels for $3 each. This means she spent $15. She has $8 left.
Q: When I was 6 my sister was half my age. Now I’m 70 how old is my sister?
A:
</div><p id="h7gh7y_159"><span class="emphasis" id="h7gh7y_160">output:</span></p><div class="code-block" data-lang="plaintext">
When I was 6 my sister was half my age, so she was 3. Now I am 70, so she is 70 - 3 = 67. The answer is 67.
</div><p id="h7gh7y_162"><span class="emphasis" id="h7gh7y_163">output 2:</span></p><div class="code-block" data-lang="plaintext">
When the narrator was 6, his sister was half his age, which is 3. Now that the narrator is 70, his sister would be 70 - 3 = 67 years old. The answer is 67.
</div><p id="h7gh7y_165"><span class="control" id="h7gh7y_166">Both answers are correct!</span> We simply add some examples as logic explanations to the prompt, then ask same question again. The model now can understand the question and answer it correctly.</p><p id="h7gh7y_167"><span class="emphasis" id="h7gh7y_168">The above example was introduced by <a href="https://www.promptingguide.ai/techniques/consistency#:~:text=Wang%20et%20al,a%20new%20tab" id="h7gh7y_169" data-external="true" rel="noopener noreferrer">Wang et al. (2022)</a>) that computing for the final answer involves a few steps.</span></p><p id="h7gh7y_170">Strong prompts can be used to guide the model to perform complex tasks, such as solving math problems or summarizing text. So <code class="code" id="h7gh7y_171">prompt engineering</code> also plays a very important role of the LLM ecosystem.</p><p id="h7gh7y_172">For more about prompt engineering, here is a good <a href="https://www.promptingguide.ai/introduction/basics" id="h7gh7y_173" data-external="true" rel="noopener noreferrer">prompting guide</a> tutorial.</p></section><div class="last-modified">Last modified: 22 April 2024</div><div data-feedback-placeholder="true"></div><div class="navigation-links _bottom"><a href="what-is-llm.html" class="navigation-links__prev">What is LLM</a><a href="intro-to-transformer.html" class="navigation-links__next">Transformer Architecture</a></div></article><div id="disqus_thread"></div></div></section></main></div><script src="https://resources.jetbrains.com/writerside/apidoc/6.6.6-b224/app.js"></script></body></html>